# 特征工程

参考链接：https://www.zhihu.com/question/28641663/answer/110165221

![img](https://pic2.zhimg.com/80/20e4522e6104ad71fc543cc21f402b36_1440w.jpg?source=1940ef5c)

#1.数据预处理

通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：

- **不属于同一量纲**：即特征的规格不一样，不能够放在一起比较(比如房价量级为数万，年龄量级为数十）。

  - **归一化**：$$x-min(x)/(max(x)-min(x))$$
  - **标准化**：$$x-mean(x)/sd(x)$$

  （一般情况下，如果对输出结果范围有要求，用归一化。如果数据较为稳定，不存在极端的最大最小值，用归一化。如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。标准化更好保持了样本间距。当样本中有异常点时，归一化有可能将正常的样本“挤”到一起去。标准化更符合统计学假。对一个数值特征来说，很大可能它是服从正态分布的。）

- **信息冗余**：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。

  - **离散化**：一般先离散化再onehot编码，可以为模型带来非线性，模型收敛快，也更加稳定，能够降低过拟合的风险。

- **类别特征不能直接使用**：某些机器学习算法和模型只能接受数值特征的输入，那么需要将类别特征转换为数值特征。

  - **Label Encoder**：为每一种类别指定一个定量值，但是这种方式过于灵活，增加了调参的工作。
  - **Onehot Encoder**：onehot编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用onehot编码后的特征可达到非线性的效果。

- **存在缺失值**：缺失值需要补充（进行补全，均值/中位数/众数/固定值）

- **信息利用率低**：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。

- **数据不均衡**：

  - 上/下采样
  - 样本设置不同权重

- **处理异常值**（基于简单统计/基于近邻/基于密度/基于聚类离群点检测）



#### 注意：树模型不需要对特征进行离散化，one-hot编码、归一化等预处理：

树模型是要寻找最佳分裂点，

+ **对于离散特征**，树模型会评估每个离散值的信息增益，将信息增益最大的数值作为分裂点，因此，树模型不需要对离散特征进行事先one-hot处理，否则会使特征维度增大且稀疏，不仅会增加模型的计算量，而且会损失数据的信息量造成模型的效果不佳，以及过拟合的风险。

+ **对于连续特征**，树模型对尝试对连续特征分桶，将信息增益最大的桶边界值最为分裂点。因此也不需要事先对连续特征进行分桶和归一化。是否对数据进行归一化，不影响最佳分裂点的计算。



#2.特征选择

特征选择是一个重要的数据预处理过程，主要有两个原因：

+ 一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;

+ 二是选取出真正相关的特征简化模型，协助理解数据产生的过程

特征选择方法分为3种：

- **Filter**：过滤法，按照发散性（方差）或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
- **Wrapper**：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
- **Embedded**：嵌入法，在前两种特征选择方法中，特征选择过程和模型训练过程是有明显分别的两个过程 嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。

## 2.1 Filter

#### 2.1.1 方差选择法

使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。

#### 2.1.2 **相关系数法**

使用相关系数法，先要计算各个特征对目标值的相关系数（**Pearson相关系数**）皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关。

#### 2.1.3 卡方检验

经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：

𝜒2=∑(𝐴−𝐸)^2/𝐸

A为实际值， E为理论值，求和值为理论值与实际值的差异程度。

基本思想是根据样本数据推断总体的分布与期望分布是否有显著性差异，或者推断两个分类变量是否相关或者独立。

#### 2.1.4 互信息法与最大信息系数

经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：

<img src="https://pic2.zhimg.com/50/6af9a077b49f587a5d149f5dc51073ba_hd.jpg?source=1940ef5c" data-rawwidth="274" data-rawheight="50" class="content_image" width="274"/>

## 2.2 Warpper

#### 2.2.1 **递归特征消除法** recursive feature elimination ( RFE )

给定一个外部的基模型**（如SVM或者回归模型）**，该基模型对特征赋予一定的权重（比如，**线性模型的系数**）。递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

RFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。

## 2.3 Embedded

#### 2.3.1 线性模型和正则化(Embedded方式)

+ **L1正则化（Lasso Regression）**

  L1正则化将系数𝑤w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。

+ **L2正则化(Ridge Regression) 岭回归**

  L2正则化同样将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，**L2正则化会让系数的取值变得平均**。



#### 2.3.2 基于树模型的特征选择法

随机森林具有准确率高、鲁棒性好、易于使用等优点，这使得它成为了目前最流行的机器学习算法之一。随机森林提供了两种特征选择的方法：平均不纯度减少(mean decrease impurity) 和 平均精确率减少 (Mean Decrease Accuracy)

+ **平均不纯度减少 (Mean Decrease Impurity)**

  随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用 **基尼不纯度** 或者 **信息增益** ，对于回归问题，通常采用的是 **方差** 或者 **最小二乘拟合**。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。

  **存在问题：**

  1. 这种方法存在偏向，对具有更多类别的变量会更有利；
  2. 对于存在_关联的多个特征_，其中任意一个都可以作为指示器（优秀的特征），并且_一旦某个特征被选择之后，其他特征的重要度就会急剧下降

+ **平均精确率减少 (Mean Decrease Accuracy)**

  另一种常用的特征选择方法就是**直接度量每个特征对模型精确率的影响**。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。



# 3.特征提取

- 降维（PCA/LDA/ICA）
- 图像特征提取
- 文本特征提取（词袋模型，ngram模型，词嵌入式模型，TFIDF）

