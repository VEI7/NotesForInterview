# 深度学习常见的优化方法(Optimizer)总结

机器学习几乎所有的算法都要利用损失函数 loss function 来检验算法模型的优劣，同时利用损失函数来提升算法模型． 这个提升的过程就叫做优化(Optimizer) 

下面这个内容主要就是介绍可以用来优化损失函数的常用方法

####**1.SGD&BGD&Mini-BGD**:

**SGD**(stochastic gradient descent):随机梯度下降,算法在每读入一个数据都会立刻计算loss function的梯度来update参数．假设loss function为L(w)，下同．

$𝑤−=𝜂▽_{𝑤𝑖}𝐿(𝑤𝑖)$ 

Pros:收敛的速度快；可以实现在线更新；

Cons:很容易陷入到局部最优，困在马鞍点．



**BGD**(batch gradient descent):批量梯度下降，算法在读取整个数据集后累加来计算损失函数的的梯度

$𝑤−=𝜂▽𝑤𝐿(𝑤)$ 

Pros:如果loss function为convex，则基本可以找到全局最优解 

Cons:数据处理量大，导致梯度下降慢;不能实时增加实例，在线更新；训练占内存



**Mini-BGD**(mini-batch gradient descent):顾名思义，选择小批量数据进行梯度下降，这是一个折中的方法．采用训练集的子集(mini-batch)来计算loss function的梯度．

$𝑤−=𝜂▽𝑤_{𝑖:𝑖+𝑛}𝐿(𝑤_{𝑖:𝑖+𝑛})$ 

这个优化方法用的也是比较多的，计算效率高而且收敛稳定，是现在深度学习的主流方法．

上面的方法都存在一个问题，就是update更新的方向完全依赖于计算出来的梯度．很容易陷入局部最优的马鞍点．能不能改变其走向，又保证原来的梯度方向．就像向量变换一样，我们模拟物理中物体流动的动量概念(惯性).引入Momentum的概念．

#### **2.Momentum**

在更新方向的时候保留之前的方向，增加稳定性而且还有摆脱局部最优的能力

Δ𝑤=𝛼Δ𝑤−𝜂▽𝐿(𝑤)

𝑤=𝑤+Δ𝑤

若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减。

#### **3.Adagrad**：(adaptive gradient)自适应梯度算法,是一种改进的随机梯度下降算法

以前的算法中，每一个参数都使用相同的学习率𝛼.但是在实际应用中，各个参数的重要性肯定是不一样的，所以我们对于不同的参数要动态的采取不同的学习率，让目标函数更快的收敛。 Adagrad算法能够在训练中自动对learning_rate进行调整，将每一个参数的每一次迭代的梯度取平方累加后再开方，用全局学习率除以这个数r，作为学习率的动态更新。从AdaGrad算法中可以看出，随着算法不断迭代，r会越来越大，整体的学习率会越来越小。所以，一般来说AdaGrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。梯度较小的参数赋予较大的学习率，梯度较大的参数赋予较小的学习率，提升在稀疏梯度上的性能。

####4.RMSprop:(root mean square propagation)

也是一种自适应学习率方法．不同之处在于，Adagrad会累加之前所有的梯度平方，RMSProp使用了一个梯度平方的滑动平均：．可以缓解Adagrad算法学习率下降较快的问题．

```text
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```

在上面的代码中，decay_rate是一个**超参数**，**常用[0.9,0.99,0.999]**。

####5.Adam:(adaptive momentum estimation)

是对RMSProp优化器的更新.利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率. 

Adam 算法同时获得了 AdaGrad 和 RMSProp 算法的优点。Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）

```text
  m = beta1*m + (1-beta1)*dx
  v = beta2*v + (1-beta2)*(dx**2)
  x += - learning_rate * m / (np.sqrt(v) + eps)
```

注意，这方法和RMSProp很像，除了使用的是平滑版的梯度m，而不是原始梯度dx。

推荐参数值eps=1e-8, beta1=0.9, beta2=0.999。

优点:每一次迭代学习率都有一个明确的范围,使得参数变化很平稳.

Adam是实际学习中最常用的算法

### Adam配置参数

- alpha也被称为学习速率或步长 learning rate。权重比例被校正(例如0.01)。
- beta1。一阶矩估计的指数衰减率(如0.9)。
- beta2。二阶矩估计的指数衰减率(例如0.999)。
- epsilon是一个非常小的数字，可以防止被0除(例如，10e-8)。



## L1不可导的时候该怎么办

当损失函数不可导,梯度下降不再有效,可以使用坐标轴下降法,梯度下降是沿着当前点的负梯度方向进行参数更新,而坐标轴下降法是沿着坐标轴的方向,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。



## sigmoid RELU  Tanh

Sigmoid函数由下列公式定义![在这里插入图片描述](https://img-blog.csdnimg.cn/20190726170302779.png)

其对x的导数可以用自身表示：处处可导

Tanh

![image-20200306212358789](/var/folders/dp/_cdw_jwj6xd7qlybydtwn6k80000gn/T/abnerworks.Typora/image-20200306212358789.png)



ReLU 的优点：

• 分段线性函数。相比于sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。采用sigmoid等函数，*算激活函数时（指数运算），计算量大*，反向传播求误差梯度时，求导涉及除法，计算量相对大。

• 无饱和问题，明显减轻梯度消失问题。sigmoid和tanh的gradient在饱和区域非常平缓，接近于0，很容易造成vanishing gradient的问题，减缓收敛速度。vanishing gradient在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一。

• 深度网络能够进行优化的关键

• 加快训练速度（相辅相成的，因为克服了梯度消失问题，所以训练才会快。）

•Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生

 

ReLU 的缺点：

训练的时候很“脆弱”，很容易就“die”了。例如当一个非常大的梯度流过一个Relu神经元时，更新参数后，这个神经元再也不会对任何数据有激活现象了。实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都“dead”了。 当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。



LeakyReLU 给所有负值赋予一个非零斜率a， 0<a<1，解决了ReLU死亡神经元的问题

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190726170436539.png)

**参数化修正线性单元（PReLU）**     PReLU可以看作是Leaky ReLU的一个变体。在PReLU中，负值部分的斜率是根据数据来定的，而非预先定义的。作者称，在ImageNet分类（2015，Russakovsky等）上，PReLU是超越人类分类水平的关键所在。

 **随机纠正线性单元（RReLU）**     “随机纠正线性单元”RReLU也是Leaky ReLU的一个变体。在RReLU中，负值的斜率在训练中是随机的，在之后的测试中就变成了固定的了。RReLU的亮点在于，在训练环节中，斜率a是从一个均匀的分布U(I,u)中随机抽取的数值。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190726170454446.png)

ELUs是“指数线性单元”，它试图将激活函数的平均值接近零，从而加快学习的速度。同时，它还能通过正值的标识来避免梯度消失的问题。根据一些研究，ELUs分类精确度是高于ReLUs的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190726170408263.png)



## 激活函数的作用

### 参考回答：

激活函数是用来加入非线性因素的,提高神经网络对模型的表达能力,解决线性模型所不能解决的问题。sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入。如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。

## 梯度消失梯度爆炸以及怎么解决

### 参考回答：

梯度消失梯度爆炸产生的原因其实就是矩阵的高次幂导致的。在多层神经网络中，梯度主要是权值和激活函数的偏导数决定。

比如一个网络结构为

$ f(x)=f_3(W_3 f_2(W_2 f_1(W_1 x)))$

对W1求梯度，根据链式求导法则，得到的解为：

$\frac {\delta f}{\delta W_1} = \frac {\delta f_3}{\delta f_2} w_3 \cdot \frac {\delta f_2}{\delta f_1} w_2 \cdot \frac {\delta f_1}{\delta w_1}$

这样可以看到，如果我们使用标准化初始w，那么各个层次的相乘都是0-1之间的小数，而激活函数f的导数也是0-1之间的数，其连乘后，结果会变的很小，导致**梯度消失**。若我们初始化的w是很大的数，w大到乘以激活函数的导数都大于1，那么连乘后，可能会导致求导的结果很大，形成**梯度爆炸**。

对于一个含有三层隐藏层的简单神经网络来说，当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习。

### 解决方案：

1）、使用 ReLU、LReLU、ELU、maxout 等激活函数

sigmoid函数的梯度随着x的增大或减小而消失，而ReLU不会。

2）、使用Batch Normalization

3）、加上残差结构 h(x) = f(x) + x

4）、梯度剪切、正则 **梯度剪切**这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。

5）LSTM通过门控机制。

## Batch Normalization的作用

神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。而Batch![img](https://uploadfiles.nowcoder.com/images/20190317/311436_1552801230080_095961B16BE2B8C2E508F4A1AB257B7D)Normalization的作用是通过标准化的手段,将越来越偏的分布拉回到正态分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。减均值除方差。从饱和区拉回到非饱和区

### 残差结构

在深层网络中，一旦其中某一层的导数很小，多次连乘后梯度可能越来越小，**这就是常说的梯度消散**，对于深层网络，传到浅层几乎就没了。但是如果使用了残差，**每一个导数就加上了一个恒等项1，dh/dx=d(f+x)/dx=1+df/dx**。此时就算原来的导数df/dx很小，这时候误差仍然能够有效的反向传播，这就是核心思想。



## 如何防止过拟合？

1.早停法；

2.l1和l2正则化；

3.神经网络的dropout；

4.决策树剪枝；

5.SVM的松弛变量；

6.集成学习：bagging(如随机森林）能有效防止过拟合

7.扩增数据集

8.特征的筛选(不是太推荐)注意：降维不能解决过拟合。降维只是减小了特征的维度，并没有减小特征所有的信息。



#### L0、L1、L2范数在机器学习中的应用

L0范数指向量中非零元素的个数 

L1范数：向量中每个元素绝对值的和 ![||x||_1=\sum_{i=1}^{N}{|x_i|}](https://math.jianshu.com/math?formula=%7C%7Cx%7C%7C_1%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7B%7Cx_i%7C%7D) 

L2范数：向量元素绝对值的平方和再开平方![||x||_2=\sqrt{\sum_{i=1}^{N}{x_i^2}}](https://math.jianshu.com/math?formula=%7C%7Cx%7C%7C_2%3D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7Bx_i%5E2%7D%7D)

L1是模型各个参数的绝对值之和,L2为各个参数平方和的开方值。L1更趋向于产生少量的特征,其它特征为0,最优的参数值很大概率出现在坐标轴上,从而导致产生稀疏的权重矩阵,而L2会选择更多的特征,但是这些特征的参数趋向于0。L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。

正则化（结构风险最小化）：为了防止过拟合提出的策略，在经验风险（训练数据集的平均损失）上加上表示模型复杂度的正则化项或罚项。结构风险最小化需要损失和模型复杂度同时小。

L1正则化：

$J=J_0+α∑_w|w|$

我们令$L=α∑_w|w|$ ,图中等值线是J0的等值线，黑色方形是L函数的图形。在图中，当J0 等值线与L图形首次相交的地方就是最优解。上图中J0与L在L的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是(w 1 ,w 2 )=(0,w)。



![image-20200307220259736](/var/folders/dp/_cdw_jwj6xd7qlybydtwn6k80000gn/T/abnerworks.Typora/image-20200307220259736.png)

L2正则化：

$J=J_0+α∑_ww^2$

二维平面下L2正则化的函数图形是个圆，与方形相比，被磨去了棱角。因此J0 与L相交时使得w1或w2等于零的机率小了许多，这就是为什么L2正则化不具有稀疏性的原因。



![image-20200307220520840](/var/folders/dp/_cdw_jwj6xd7qlybydtwn6k80000gn/T/abnerworks.Typora/image-20200307220520840.png)

L2正则化和过拟合
拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。

最开始也提到L1正则化一定程度上也可以防止过拟合。之前做了解释，当L1的正则化系数很大时，得到的最优解会很小，可以达到和L2正则化类似的效果。

推导过程参考：<https://blog.csdn.net/jinping_shi/article/details/52433975>



#### 问题：Loss Function有哪些，怎么用？

平方损失（预测问题）、交叉熵（分类问题）、hinge损失（SVM支持向量机）、CART回归树的残差损失





## 正负样本不平衡的解决办法？评价指标的参考价值？

### 参考回答：

上下采样法。

好的指标：ROC和AUC、F值、G-Mean；不好的指标：Precision、Recall



## 迁移学习

迁移学习就是把之前训练好的模型直接拿来用，可以充分利用之前数据信息，而且能够避免自己实验数据量较小等问题。简单来讲就是给模型做初始化，初始化的数据来自于训练好的模型。



## 特征工程

+ 数据预处理

  + 处理缺失值（进行补全，均值/中位数/众数/固定值。。。补全）
  + 数据扩充
  + 处理样本类别不均衡（上下采样）
  + 处理异常值（基于简单统计/基于近邻/基于密度/基于聚类离群点检测）

+ 特征选择

+ 特征提取

  + 降维（PCA/LDA/ICA）
  + 图像特征提取
  + 文本特征提取（词袋模型，ngram模型，词嵌入式模型，TFIDF）

+ 特征构建（特征构建是指**从原始数据中人工的构建新的特征**）


## 特征选择怎么做

特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解。

常见的特征选择方式：

1）、去除方差较小的特征

2）、正则化。L1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。

3）、随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。

4）、稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。





## 训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些?

### 参考回答：

并不能说明这个模型无效,导致模型不收敛的原因可能有数据分类的标注不准确,样本的信息量太大导致模型不足以fit整个样本空间。学习率设置的太大容易产生震荡,太小会导致不收敛。可能复杂的分类任务用了简单的模型。数据没有进行归一化的操作。



## 什么是dropout

### 参考回答：

在神经网络的训练过程中,对于神经单元按一定的概率将其随机从网络中丢弃,从而达到对于每个mini-batch都是在训练不同网络的效果,防止过拟合。

## 怎么提升网络的泛化能力

### 参考回答：

从数据上提升性能:收集更多的数据,对数据做缩放和变换,特征组合和重新定义问题。

从算法调优上提升性能:用可靠的模型诊断工具对模型进行诊断,权重的初始化,用小的随机数初始化权重。对学习率进行调节,尝试选择合适的激活函数,调整网络的拓扑结构,调节batch和epoch的大小,添加正则化的方法,尝试使用其它的优化方法,使用early stopping。





## Focal Loss 介绍一下

### 参考回答：

Focal loss主要是为了解决one-stage目标检测中正负样本比例严重失衡的问题。该损失函数降低了大量简单负样本在训练中所占的权重，也可理解为一种困难样本挖掘。

损失函数形式：Focal loss是在交叉熵损失函数基础上进行的修改，首先回顾二分类交叉上损失：

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552657534871_E429E38DB287AE3F7F68AD1172A48606)

是经过激活函数的输出，所以在0-1之间。可见普通的交叉熵对于正样本而言，输出概率越大损失越小。对于负样本而言，输出概率越小则损失越小。此时的损失函数在大量简单样本的迭代过程中比较缓慢且可能无法优化至最优。

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552657519849_21259D5A3307E27C1AD9AAC6160B05CA)

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552657505129_0D7FBDDC029A298D16708726E43FB2E7)

首先在原有的基础上加了一个因子，其中gamma>0使得减少易分类样本的损失。使得更关注于困难的、错分的样本。

例如gamma为2，对于正类样本而言，预测结果为0.95肯定是简单样本，所以（1-0.95）的gamma次方就会很小，这时损失函数值就变得更小。而预测概率为0.3的样本其损失相对很大。对于负类样本而言同样，预测0.1的结果应当远比预测0.7的样本损失值要小得多。对于预测概率为0.5时，损失只减少了0.25倍，所以更加关注于这种难以区分的样本。这样减少了简单样本的影响，大量预测概率很小的样本叠加起来后的效应才可能比较有效。

此外，加入平衡因子alpha，用来平衡正负样本本身的比例不均：

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552657490617_00DB87D29DA5A959E603CF06BC578795)

只添加alpha虽然可以平衡正负样本的重要性，但是无法解决简单与困难样本的问题。

lambda调节简单样本权重降低的速率，当lambda为0时即为交叉熵损失函数，当lambda增加时，调整因子的影响也在增加。实验发现lambda为2是最优。
