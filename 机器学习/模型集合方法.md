## 模型集合方法

集成学习是一种机器学习范式。在集成学习中，我们会训练多个模型（通常称为「弱学习器」）解决相同的问题，并将它们结合起来以获得更好的结果。最重要的假设是：当弱模型被正确组合时，我们可以得到更精确和/或更鲁棒的模型。

这就引出了如何组合这些模型的问题。我们可以用三种主要的旨在组合弱学习器的「元算法」：

+ bagging，该方法通常考虑的是同质弱学习器，相互独立地并行学习这些弱学习器，并按照某种确定性的平均过程将它们组合起来。

+ boosting，该方法通常考虑的也是同质弱学习器。它以一种高度自适应的方法顺序地学习这些弱学习器（每个基础模型都依赖于前面的模型），并将它们按照权重线性组合起来。

+ stacking，该方法通常考虑的是异质弱学习器，并行地学习它们，并通过训练一个「元模型」将它们组合起来，根据不同弱模型的预测结果输出一个最终的预测结果。



Bagging即套袋法，其算法过程如下：

1. 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
2. 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
3. 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）



AdaBoost方式每次使用的是全部的样本，每轮训练改变样本的权重。Boosting会减小在上一轮训练正确的样本的权重，增大错误样本的权重。AdaBoost采取加权多数表决的方法组合弱学习器。即加大分类误差率小的弱学习器的权重，使其在表决中起较大的作用。Adaboost是模型为加法模型，损失函数为指数函数，学习算法为前向分步算法时的二分类学习算法。前向分步算法的想法是：因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式。把同时求解从m=1到M的所有参数的优化问题简化为逐次求解各个基函数参数的优化问题。而对于回归问题，则是拟合数据的残差。

梯度提升的Boosting方式是使用代价函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树。



Bagging和Boosting的区别：

1）样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：Bagging：使用均匀取样，每个样例的权重相等。Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：Bagging：所有预测函数的权重相等。Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：Bagging：各个预测函数可以并行生成。Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

5）bagging是减少variance，而boosting是减少bias



#### GBDT（Gradient Boosting Decision Tree）

GBDT是梯度提升决策树（Gradient Boosting Decision Tree）的简称，GBDT可以说是最好的机器学习算法之一。**GBDT分类和回归时的基学习器都是CART回归树，因为是拟合残差的。**GBDT和Adaboost一样可以用前向分步算法来描述，不同之处在于Adaboost算法每次拟合基学习器时，输入的样本数据是不一样的（每一轮迭代时的样本权重不一致），因为Adaboost旨在重点关注上一轮分类错误的样本，GBDT算法在每一步迭代时是输出的值不一样，本轮要拟合的输出值是之前的加法模型的预测值和真实值的差值（模型的残差，也称为损失）。用于一个简单的例子来说明GBDT，假如某人的年龄为30岁，第一次用20岁去拟合，发现损失还有10岁，第二次用6岁去拟合10岁，发现损失还有4岁，第三次用3岁去拟合4岁，依次下去直到损失在我们可接受范围内。

#### XGBoost

和GBDT方法一样，XGBoost的提升模型也是采用残差，不同的是分裂结点选取的时候不一定是最小平方损失，其损失函数如下，较GBDT其根据树模型的复杂度加入了一项正则化项：

在GBDT中我们通过求损失函数的负梯度（一阶导数），利用负梯度替代残差来拟合树模型。在XGBoost中直接用泰勒展开式将损失函数展开成二项式函数（**前提是损失函数一阶、二阶都连续可导，而且在这里计算一阶导和二阶导时可以并行计算**



**XGBoost和GBDT的区别**

　  1）GDBT在函数空间中利用梯度下降法进行优化而XGB在函数空间中使用了牛顿法进行优化。

　  2）XGBoost损失函数是用泰勒展开式展开的，同时用到了一阶导和二阶导，可以加快优化速度。

　　3）XGB在损失函数中加入了正则项(树叶子节点个数,每个叶子节点上输出score的L2模平方和。

　　4）引进了特征子采样，像RandomForest那样，这种方法既能降低过拟合，还能减少计算。

　　5）在寻找最佳分割点时，考虑到传统的贪心算法效率较低，实现了一种近似贪心算法，用来加速和减小内存消耗，除此之外还考虑了稀疏数据集和缺失值的处理，对于特征的值有缺失的样本，XGBoost依然能自动找到其要分裂的方向。

　　6）XGBoost支持并行处理，XGBoost的并行不是在模型上的并行，而是在特征上的并行，将特征列排序后以block的形式存储在内存中，在后面的迭代中重复使用这个结构。这个block也使得并行化成为了可能，其次在进行节点分裂时，计算每个特征的增益，最终选择增益最大的那个特征去做分割，那么各个特征的增益计算就可以开多线程进行。

####GBDT的原理,以及常用的调参参数

先用一个初始值去学习一棵树,然后在叶子处得到预测值以及预测值和真实值的差值,之后的树则基于之前树的残差不断的拟合得到,从而训练出一系列的树作为模型。

n_estimators基学习器的最大迭代次数,learning_rate学习率，max_lead_nodes最大叶子节点数,max_depth树的最大深度,min_samples_leaf叶子节点上最少样本数。

## 随机森林和 GBDT 的区别

1）随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。2）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。3）组成随机森林的树可以并行生成；而GBDT只能是串行生成。4）对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果加权累加起来。5）随机森林对异常值不敏感；GBDT对异常值非常敏感。6）随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。7）随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。



## 问题：Bootstrap方法是什么？

### 参考回答：

从一个数据集中有放回的抽取N次，每次抽M个。

解析：Bagging算法基于bootstrap。面试时结合Bagging算法讲述会更好。

