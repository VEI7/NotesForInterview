## 分类问题

###LR

 LR属于对数线性模型

二项逻辑斯蒂回归模型是一种分类模型，由条件概率分布$P(Y/X)$ 表示，形式为参数化的逻辑斯蒂分布。这里随机变量X为实数，随机变量Y取值为1或0。

$ P(Y=1|x) = \frac {exp(w \cdot x+b)}{1+ exp(w \cdot x+b)}$

$ P(Y=0|x) = \frac {1}{1+ exp(w \cdot x+b)}$

线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0，这样的模型就是LR模型。

LR模型学习时，可以用极大似然估计法估计模型参数。（线性回归使用最小二乘法）

设：

$ P(Y=1|x) = \pi (x) $

$ P(Y=0|x) = 1 - \pi(x)$  

似然函数为：

$$ \prod_{i=1}^{N} [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

对数似然函数为：

$$ L(w) = \sum_{i=1}^{N} [y_i log \pi(x_i) + (1-y_i) log(1-\pi(x_i))]$$

对L(w)求极大值，得到w的估计值

### SVM

支持向量机为一个二分类模型,它的基本模型定义为特征空间上的间隔最大的线性分类器。而它的学习策略为最大化分类间隔,最终可转化为凸二次规划问题求解，也等价于正则化的hinge损失函数的最小化问题。

SVM分为如下三类：

+ 线性可分支持向量机：当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器。

+ 线性支持向量机：当训练数据近似线性可分时，通过软间隔最大化，学习一个线性的分类器。

+ 非线性支持向量机：当训练数据不可分时，通过核函数学习非线性分类器。（核函数表示将输入从输入空间映射到特征空间得到特征向量之间的内积。通过核函数学习非线性支持向量机，等价于在高维的特征空间学习线性支持向量机）


考虑一个二分类问题，假设输入空间与特征空间为两个不同的空间。线性可分支持向量机和线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射到特征空间中的特征向量。而非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。**所以输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间中进行的。**

#####线性可分支持向量机

学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应的方程为：$w \cdot x + b=0$

函数间隔：(点（$x_i, y_i$)与超平面的距离）

$\gamma^` = y_i(w \cdot x_i + b)$

$y_i$ = 1 or -1

如果我们将w和b改为2w和2b，超平面没有改变，函数间隔成了原来的两倍。那么我们可以对分离超平面的法向量w加些约束，如规范化， 即$||w||=1$，使得间隔时确定的，这时函数间隔就成了几何间隔：

$\gamma = y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||})$

间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例分开，而且对最难分的实例点也有足够大的确信度将它们分开。

##### 线性支持向量机与软间隔最大化

线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件，为了解决这个问题，可以对每个样本点$(x_i,y_i)$引入一个松弛变量$\xi_i>=0$，使函数间隔加上松弛变量大于等于1.这样约束条件就变成了

$y_i(w \cdot x_i + b) \ge 1-\xi_i$

目标函数变为了：

$min \ \frac{1}{2}||w||^2 + C\sum_{i=1}^{N}{\xi_i}$

等价于最优化问题：

$min \ \lambda||w||^2 + \sum_{i=1}^{N}{[1-y_i(w \cdot x_i + b)]_+}$

##### 非线性支持向量机与核函数

核函数：通过一个非线性变换将输入空间对应于特征空间，使得在输入空间中的超曲面模型对应于特征空间中的超平面模型，这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。

###

#### 2.什么是支持向量机,SVM与LR的区别?

SVM可以用于解决二分类或者多分类问题，此处以二分类为例。SVM的目标是寻找一个最优化超平面在空间中分割两类数据，这个最优化超平面需要满足的条件是：离其最近的点到其的距离最大化，这些点被称为支持向量。

支持向量机为一个二分类模型,它的基本模型定义为特征空间上的间隔最大的线性分类器。而它的学习策略为最大化分类间隔,最终可转化为凸二次规划问题求解。

LR是参数模型,SVM为非参数模型。是否为参数模型其区别主要在于总体的分布形式是否已知。而为何强调“参数”与“非参数”，主要原因在于参数模型的分布可以由参数直接确定。对于目标函数形式不作过多的假设的算法称为非参数机器学习算法。通过不做假设，算法可以自由的从训练数据中学习任意形式的函数。

LR采用的损失函数为logistical loss,而SVM采用的是hinge loss。在学习分类器的时候,SVM只考虑与分类最相关的少数支持向量点。LR的模型相对简单,在进行大规模线性分类时比较方便。

Hinge Loss 是机器学习领域中的一种损失函数，可用于“最大间隔(max-margin)”分类，其最著名的应用是作为SVM的目标函数。 

在二分类情况下，公式如下： 

*L*(y) = max(0 , 1 – t⋅y)

其中，y是预测值(-1到1之间)，t为目标值(1或 -1)。当样本与分割线的距离超过1时并不会有任何奖励。目的在于使分类器更专注于整体的分类误差。



###常见的回归损失函数有

- **平方损失 (squared loss)** ： (𝑦−𝑓(𝑥))^2
- **绝对值 (absolute loss)** : |𝑦−𝑓(𝑥)|

### 分类问题的损失函数

**1、 0-1损失 (zero-one loss)**



	𝐿(𝑦,𝑓(𝑥))=  0 if 𝑦𝑓(𝑥)≥0
				1 if 𝑦𝑓(𝑥)<0

0-1损失对每个错分类点都施以相同的惩罚，这样那些“错的离谱“ (即 𝑚𝑎𝑟𝑔𝑖𝑛→−∞)的点并不会收到大的关注，这在直觉上不是很合适。另外0-1损失不连续、非凸，优化困难，因而常使用其他的代理损失函数进行优化。

**2、Logistic loss(交叉熵损失函数/对数似然损失函数)**

$ L_i = -[y_i * log(y'_i) +(1-y_i)* log(1-y'_i) ]$

y为ground truth， y'为预测值

当y=1时， $ L_i = - log(y'_i) $

当y=0时， $ L_i = - log(1-y'_i) $



多分类交叉熵损失函数（softmax）$ L = \sum_{j}- y_jlog(y'_j) $

 其中j为类别的下标，$y_j$只有一个元素为1，其他皆为0。y'为预测值的softmax值。

L的导数=y‘-1.  我们只需要求出y‘ ，将结果减1就是反向更新的梯度，导数的计算是不是非常简单！


**3、Hinge loss(SVM)**

𝐿(𝑦,𝑓(𝑥))=𝑚𝑎𝑥(0,1−𝑦𝑓(𝑥)) 



### 二分类/多分类/多标签分类

		  	输出层使用激活函数			对应的损失函数

二分类：         sigmoid						二分类交叉熵损失函数

多分类：         softmax						多分类交叉熵损失函数

多标签分类： sigmoid						二分类交叉熵损失函数





#### 3.向量距离

欧式距离：

![img](https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D191/sign=7bb3375e3c4e251fe6f7e0f19687c9c2/e7cd7b899e510fb335a3e2f3d533c895d1430c1f.jpg)

曼哈顿距离

$d（i，j）=|x_i-x_j|+|y_i-y_j|$

余弦距离：

$cos=\frac {x_1*y_1+x_2*y_2}{\sqrt{x_1^2+y_1^2} + \sqrt{x_2^2+y_2^2}}$



#### 4.朴素贝叶斯（naive Bayes）法的要求是？

贝叶斯定理、特征条件独立假设

解析：朴素贝叶斯属于生成式模型，学习输入和输出的联合概率分布。给定输入x，利用贝叶斯概率定理求出最大的后验概率作为输出y。

#### 5如果给你一些数据集，你会如何分类（我是分情况答的，从数据的大小，特征，是否有缺失，分情况分别答的）

根据数据类型选择不同的模型，如Lr或者SVM，决策树。假如特征维数较多，可以选择SVM模型，如果样本数量较大可以选择LR模型，但是LR模型需要进行数据预处理；假如缺失值较多可以选择决策树。选定完模型后，相应的目标函数就确定了。还可以在考虑正负样例量比，通过上下集采样平衡正负样例比。采样分为上采样（Oversampling）和下采样（Undersampling），上采样是把小众类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本。或者通过给正负例样本赋予不同的权重，小众样本惩罚大一些。

解析：需要了解多种分类模型的优缺点，以及如何构造分类模型的步骤

#### 6.LR和线性回归的区别

线性回归用来做预测,LR用来做分类。线性回归是来拟合函数,LR是来预测函数。线性回归用最小二乘法来计算参数,LR用最大似然估计来计算参数。线性回归更容易受到异常值的影响,而LR对异常值有较好的稳定性。



### Precision/ Recall/ F1/ Micro-F1 Macro-F1

对于数据测试结果有下面4种情况：

真阳性（TP）: 预测为正， 实际也为正

假阳性（FP）: 预测为正， 实际为负

假阴性（FN）: 预测为负，实际为正

真阴性（TN）: 预测为负， 实际也为负

准确率（P）： TP/ (TP+FP) 

召回率(R)： TP(TP + FN) 

**F1-score:    2\*(P*R)/(P+R)**

**'micro':**Calculate metrics globally by counting the total true positives, false negatives and false positives.

'micro':通过先计算总体的TP，FN和FP的数量，再计算F1

**'macro':**Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.

'macro':分布计算每个类别的F1，然后做平均（各类别F1的权重相同）





## ROC AUC

TPR=TP/(TP+FN) 表示的就是预测正确且实际分类为正的数量与所有正样本的数量的比例。--实际的正样本中，正确预测的比例是多少

FPR=FP/(FP+TN) 表示的是预测错误且实际分类为负的数量 与所有负样本数量的比例。 --实际的负样本当中，错误预测的比例是多少

以TPR为纵坐标，FPR为横坐标画图，可以得到ROC曲线

AUC(Area under Curve)：Roc曲线下的面积，介于0.1和1之间。Auc作为数值可以直观的评价分类器的好坏，值越大越好。首先AUC值是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。

ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。 
